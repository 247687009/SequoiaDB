<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_25h_znr_fm">
  <title>Integration with MapReduce</title>  
  <body>
    <section>
      <title>Build Hadoop Environment</title>
      <p>We support both Hadoop 1.x and Hadoop 2.x, please install and configure Hadoop first.</p>
    </section>
    <section>
      <title>Configure Docking Environment</title>
      <p>hadoop-connector.jar and sequoiadb.jar are used for the docking with MapReduce. These two jar files can be found under the hadoop directory of the SequoiaDB installation directory.</p>
      <p>We need to check the classpath of Hadoop first because it may vary in different versions. Enter hadoop classpath, select one directory form the classpath, move haddp-connector.jar and sequoiadb.jar into the directory. Restart the hadoop cluster.</p>
    </section>
    <section>
      <title>Write MapReduce</title>
      <p><b>Some important classes in hadoop-connector.jar:</b></p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;SequoiadbInputFormat: reads data from SequoiaDB</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;SequoiadbOutputFormat: writes data into SequoiaDB</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;BSONWritable: BSONObject's wrapper class, it realizes the interfaces of WritableComparable class. Used to serialize BSONObject objects.</p>
      <p><b>The Configuration of SequoiaDB &amp; MapReduce</b></p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;Put the configuration file named sequoiadb-hadoop.xml into the root directory of the source code of the project.</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.input.url: Specify the URL of the SequoiaDB which acts as an input source, the format is: hostname1:port1, hostname2:port2,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.in.collectionspace: Specify the collection space which acts as an input source.</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.in.collection: Specify the collection which acts as an input source.</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.output.url: Specify the URL of the SequoiaDB which acts as an output target.</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.out.collectionspace: Specify the collection space which acts as an output target.</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.out.collection: Specify the collection which acts as an output target.</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sequoiadb.out.bulknum: Specify the number of records that are written into SequoiaDB for each time in order to optimize the write performance.</p>
      <p><b>Examples</b></p>
      <ul>
        <li>
      
      <p>1. The following codes read and process the data form HDFS files, and then write the result into SequoiaDB.</p>
      <codeblock>public class HdfsSequoiadbMR {
    static class MobileMapper extends  Mapper&lt;LongWritable,Text,Text,IntWritable>{
        private static final IntWritable ONE=new IntWritable(1);
        @Override
        protected void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {
            String valueStr=value.toString();
            
            String mobile_prefix=valueStr.split(",")[3].substring(0,3);
            context.write(new Text(mobile_prefix), ONE);
        }
        
    }
    
    static class MobileReducer extends Reducer&lt;Text, IntWritable, NullWritable, BSONWritable>{

        @Override
        protected void reduce(Text key, Iterable&lt;IntWritable> values,Context context)
                throws IOException, InterruptedException {
                Iterator&lt;IntWritable> iterator=values.iterator();
                long sum=0;
                while(iterator.hasNext()){
                    sum+=iterator.next().get();
                }
                BSONObject bson=new BasicBSONObject();
                bson.put("prefix", key.toString());
                bson.put("count", sum);
                context.write(null,new BSONWritable(bson));
        }
        
    }
    
    
    
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        if(args.length&lt;1){
            System.out.print("please set input path ");
            System.exit(1);
        }
        Configuration conf=new Configuration();
        conf.addResource("sequoiadb-hadoop.xml"); //load the configuration file
        Job job=Job.getInstance(conf);
        job.setJarByClass(HdfsSequoiadbMR.class);
        job.setJobName("HdfsSequoiadbMR");
        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(SequoiadbOutputFormat.class); //the reduce oupput is written to SequoiaDB
        TextInputFormat.setInputPaths(job, new Path(args[0]));

        job.setMapperClass(MobileMapper.class);    
        job.setReducerClass(MobileReducer.class);
        
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        
        job.setOutputKeyClass(NullWritable.class);        
        job.setOutputValueClass(BSONWritable.class);
        
        job.waitForCompletion(true);
    }
}</codeblock></li>
      <li><p>2. Reads and processes the data from SequoiaDB, and then writes the result into HDFS.</p>
      <codeblock>public class SequoiadbHdfsMR {
    /**
     * 
     * @author gaoshengjie
     *  read the data, count penple in a province
     */
    static class ProvinceMapper extends Mapper&lt;Object, BSONObject,IntWritable,IntWritable>{
        private static final IntWritable ONE=new IntWritable(1);
        @Override
        protected void map(Object key, BSONObject value, Context context)
                throws IOException, InterruptedException {
            int province=(Integer) value.get("province_code");
            context.write(new IntWritable(province), ONE);
        }
            
    }
    
    static class ProvinceReducer extends Reducer&lt;IntWritable,IntWritable,IntWritable,LongWritable>{

        @Override
        protected void reduce(IntWritable key, Iterable&lt;IntWritable> values,
                Context context)
                throws IOException, InterruptedException {
            Iterator&lt;IntWritable> iterator=values.iterator();
            long sum=0;
            while(iterator.hasNext()){
                sum+=iterator.next().get();
            }
            context.write(key,new LongWritable(sum));
        }

    }
    
    
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        if(args.length&lt;1){
            System.out.print("please set  output path ");
            System.exit(1);
        }
        Configuration conf=new Configuration();
        conf.addResource("sequoiadb-hadoop.xml");
        Job job=Job.getInstance(conf);
        job.setJarByClass(SequoiadbHdfsMR.class);
        job.setJobName("SequoiadbHdfsMR");
        job.setInputFormatClass(SequoiadbInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        
        FileOutputFormat.setOutputPath(job, new Path(args[0]+"/result"));
        
        job.setMapperClass(ProvinceMapper.class);    
        job.setReducerClass(ProvinceReducer.class);
        
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);
        
        job.setOutputKeyClass(IntWritable.class);        
        job.setOutputValueClass(LongWritable.class);
        
        job.waitForCompletion(true);
    }
}</codeblock></li>
      </ul>
      <p>configuration file:</p>
      <codeblock>&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;configuration>
  &lt;property>
     &lt;name>sequoiadb.input.url&lt;/name>
     &lt;value>localhost:11810&lt;/value>
  &lt;/property>
  &lt;property>
     &lt;name>sequoiadb.output.url&lt;/name>
     &lt;value>localhost:11810&lt;/value>
  &lt;/property>
  &lt;property>
     &lt;name>sequoiadb.in.collectionspace&lt;/name>
     &lt;value>default&lt;/value>
  &lt;/property>
  &lt;property>
     &lt;name>sequoiadb.in.collect&lt;/name>
     &lt;value>student&lt;/value>
  &lt;/property>
  &lt;property>
     &lt;name>sequoiadb.out.collectionspace&lt;/name>
     &lt;value>default&lt;/value>
  &lt;/property>
  &lt;property>
     &lt;name>sequoiadb.out.collect&lt;/name>
     &lt;value>result&lt;/value>
  &lt;/property>
    &lt;property>
     &lt;name>sequoiadb.out.bulknum&lt;/name>
     &lt;value>10&lt;/value>
  &lt;/property>
&lt;/configuration></codeblock>
    </section>
  </body>
</topic>
